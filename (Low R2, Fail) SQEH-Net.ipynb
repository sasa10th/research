{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnHF96XxFvN16OE2Q0B5bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sasa10th/research/blob/main/research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQEH-Net(경량화)"
      ],
      "metadata": {
        "id": "7_Q0Ja-EqNd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코드"
      ],
      "metadata": {
        "id": "OpVe0OQmqTtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72vSor5FtGdP",
        "outputId": "97aae957-b3fd-4c84-f99f-a037d91f5f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train MSE 1.0956 | test R² -0.0666 | best -0.0666\n",
            "Epoch 002 | train MSE 1.1287 | test R² -0.0052 | best -0.0052\n",
            "Epoch 003 | train MSE 1.1145 | test R² -0.0321 | best -0.0052\n",
            "Epoch 004 | train MSE 1.0729 | test R² -0.0515 | best -0.0052\n",
            "Epoch 005 | train MSE 1.0930 | test R² -0.0346 | best -0.0052\n",
            "Epoch 006 | train MSE 1.0912 | test R² -0.0644 | best -0.0052\n",
            "Epoch 007 | train MSE 1.0496 | test R² -0.0589 | best -0.0052\n",
            "Epoch 008 | train MSE 1.0986 | test R² -0.0082 | best -0.0052\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "# =============================================================================\n",
        "# Utility – 재현성 설정\n",
        "# =============================================================================\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"모든 난수 시드 설정 (reproducibility 확보용)\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Sparse-Binary Linear Layer 정의 (STE + Dynamic Sparsity 지원)\n",
        "# =============================================================================\n",
        "class SparseBinaryLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    고정된 sparsity mask를 갖는 binary weight 기반의 Linear 계층.\n",
        "\n",
        "    - 학습 시에는 실수 weight (`w_real`)에 대해 최적화.\n",
        "    - 순전파 시에는 이진화된 sign(weight)에 mask를 적용하여 연산 수행.\n",
        "    - STE (Straight-Through Estimator) 방식으로 gradient 전달.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, density: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.density = density\n",
        "\n",
        "        # 초기화: 작은 실수값으로 초기화된 학습 가능 파라미터\n",
        "        self.w_real = nn.Parameter(0.01 * torch.randn(out_features, in_features))\n",
        "\n",
        "        # sparsity mask 초기화 (density 비율로 랜덤 연결 선택)\n",
        "        num_active = int(in_features * out_features * density)\n",
        "        mask = torch.zeros(out_features, in_features, dtype=torch.bool)\n",
        "        idx = torch.randperm(mask.numel())[:num_active]\n",
        "        mask.view(-1)[idx] = True\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    @staticmethod\n",
        "    def _binarise(w: torch.Tensor):\n",
        "        \"\"\"STE 기반 sign 함수 구현\"\"\"\n",
        "        return (w >= 0).float() * 2 - 1 + w - w.detach()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        w_bin = self._binarise(self.w_real) * self.mask\n",
        "        return nn.functional.linear(x, w_bin)\n",
        "\n",
        "    def prune_and_grow(self, grow_fraction: float = 0.05):\n",
        "        \"\"\"\n",
        "        동적 sparsity:\n",
        "        - 가장 작은 magnitude의 활성 weight 일부 제거 (prune)\n",
        "        - 같은 수만큼 비활성 위치에 새 weight 추가 (grow)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            active_weights = self.w_real.abs() * self.mask\n",
        "            k_prune = int(self.mask.sum().item() * grow_fraction)\n",
        "            if k_prune == 0:\n",
        "                return\n",
        "            prune_vals, prune_idx = torch.topk(active_weights.view(-1), k_prune, largest=False)\n",
        "            flat_mask = self.mask.view(-1)\n",
        "            flat_mask[prune_idx] = False\n",
        "\n",
        "            inactive_idx = (~flat_mask).nonzero(as_tuple=False).squeeze()\n",
        "            grow_idx = inactive_idx[torch.randperm(len(inactive_idx))[:k_prune]]\n",
        "            flat_mask[grow_idx] = True\n",
        "            self.w_real.view(-1)[grow_idx] = 0.01 * torch.randn_like(self.w_real.view(-1)[grow_idx])\n",
        "\n",
        "# =============================================================================\n",
        "# 2. SQEH 네트워크 정의\n",
        "# =============================================================================\n",
        "def build_sqeh_net(n_in: int, hidden: int = 64, density: float = 0.2, beta: float = 0.95):\n",
        "    \"\"\"Sparse-Binary + LIF 뉴런 기반 네트워크 생성\"\"\"\n",
        "    return nn.ModuleDict({\n",
        "        \"fc1\": SparseBinaryLinear(n_in, hidden, density),\n",
        "        \"lif1\": snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid()),\n",
        "        \"fc2\": SparseBinaryLinear(hidden, 1, density),\n",
        "    })\n",
        "\n",
        "def forward_seq(net: nn.ModuleDict, spike_seq: torch.Tensor):\n",
        "    \"\"\"\n",
        "    시퀀스 기반 순전파 수행 (stateless, 마지막 time step 결과만 반환)\n",
        "    \"\"\"\n",
        "    mem = None\n",
        "    for t in range(spike_seq.size(0)):\n",
        "        x_t = spike_seq[t]\n",
        "        h1 = net[\"fc1\"](x_t)\n",
        "        spk, mem = net[\"lif1\"](h1, mem)\n",
        "        out = net[\"fc2\"](spk)\n",
        "    return out.squeeze(1)  # shape: (B,)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 학습 도우미 클래스\n",
        "# =============================================================================\n",
        "def latency_encode(x_batch: torch.Tensor, n_steps: int):\n",
        "    \"\"\"\n",
        "    Latency encoding:\n",
        "    입력값이 클수록 빠르게 스파이크 발생 → time dimension 생성\n",
        "    \"\"\"\n",
        "    batch, feat = x_batch.shape\n",
        "    latency = ((n_steps - 1) * (1.0 - x_batch)).round().long()\n",
        "    spike_seq = torch.zeros(n_steps, batch, feat, device=x_batch.device)\n",
        "    for t in range(n_steps):\n",
        "        spike_seq[t][latency == t] = 1.0\n",
        "    return spike_seq\n",
        "\n",
        "class SqehTrainer:\n",
        "    \"\"\"\n",
        "    SQEH 네트워크 학습기:\n",
        "    - spike 기반 학습 + reward 기반 weight sign flip\n",
        "    - dynamic pruning/growing 포함\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, net, lr=1e-3, time_steps=10, device=\"cpu\"):\n",
        "        self.net = net.to(device)\n",
        "        self.time_steps = time_steps\n",
        "        self.device = device\n",
        "        self.opt = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
        "        self.criterion = nn.MSELoss(reduction=\"none\")  # per-sample 손실 계산용\n",
        "\n",
        "    def train_epoch(self, loader, prune_every=10, grow_frac=0.05):\n",
        "        self.net.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for i, (x, y) in enumerate(loader, 1):\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "            # --- 순전파 & 역전파 ---\n",
        "            spk_seq = latency_encode(x, self.time_steps)\n",
        "            y_pred = forward_seq(self.net, spk_seq)\n",
        "            loss = self.criterion(y_pred, y).mean()\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "            # --- 보상 기반 sign flip ---\n",
        "            with torch.no_grad():\n",
        "                spk_seq2 = latency_encode(x, self.time_steps)\n",
        "                y_pred_no_grad = forward_seq(self.net, spk_seq2)\n",
        "                per_sample_error = (y_pred_no_grad - y).pow(2)\n",
        "                R = torch.from_numpy(-(per_sample_error.cpu().numpy() - per_sample_error.mean().item())).float().to(self.device)\n",
        "\n",
        "                for layer in [self.net[\"fc1\"], self.net[\"fc2\"]]:\n",
        "                    prob = (R.unsqueeze(1) * 0.1).sigmoid()\n",
        "                    flip_prob = prob.mean().item()\n",
        "                    rand_mat = torch.rand_like(layer.w_real)\n",
        "                    flip_mask = (rand_mat < flip_prob) & layer.mask\n",
        "                    layer.w_real[flip_mask] *= -1\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # --- 동적 sparsity (prune + grow) ---\n",
        "            if i % prune_every == 0:\n",
        "                for layer in [self.net[\"fc1\"], self.net[\"fc2\"]]:\n",
        "                    layer.prune_and_grow(grow_frac)\n",
        "\n",
        "        return total_loss / len(loader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, loader):\n",
        "        \"\"\"MSE, RMSE, R² 계산\"\"\"\n",
        "        self.net.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "            spk_seq = latency_encode(x, self.time_steps)\n",
        "            pred = forward_seq(self.net, spk_seq)\n",
        "            y_true.append(y.cpu())\n",
        "            y_pred.append(pred.cpu())\n",
        "        y_true = torch.cat(y_true).numpy()\n",
        "        y_pred = torch.cat(y_pred).numpy()\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        return mse, np.sqrt(mse), r2_score(y_true, y_pred)\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Tabular 데이터셋 래퍼\n",
        "# =============================================================================\n",
        "class TabularDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Numpy → Tensor 변환 및 인덱싱 지원\"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Main 실행부\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"cleaned_data.xlsx\"  # 데이터 파일 경로\n",
        "    TEST_SPLIT = 0.25\n",
        "    BATCH_SIZE = 32\n",
        "    T_STEPS = 16\n",
        "    DENSITY = 0.2  # 전체 연결 중 20%만 활성화\n",
        "\n",
        "    set_seed(7)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- 데이터 로딩 ---\n",
        "    df = pd.read_excel(DATA_PATH)\n",
        "    y = df.iloc[:, 0].to_numpy(dtype=np.float32)\n",
        "    X = df.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
        "\n",
        "    # --- 정규화 ---\n",
        "    x_scaler = MinMaxScaler()\n",
        "    y_scaler = StandardScaler()\n",
        "    X_scaled = x_scaler.fit_transform(X)\n",
        "    y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_scaled, test_size=TEST_SPLIT, random_state=42\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(TabularDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(TabularDataset(X_test, y_test), batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- 네트워크 및 트레이너 초기화 ---\n",
        "    net = build_sqeh_net(n_in=X.shape[1], hidden=64, density=DENSITY).to(device)\n",
        "    trainer = SqehTrainer(net, lr=1e-3, time_steps=T_STEPS, device=device)\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    n_epochs = 100\n",
        "    best_r2 = -np.inf\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = trainer.train_epoch(train_loader)\n",
        "        mse, rmse, r2 = trainer.evaluate(test_loader)\n",
        "        best_r2 = max(best_r2, r2)\n",
        "        print(f\"Epoch {epoch:03d} | train MSE {train_loss:.4f} | test R² {r2:.4f} | best {best_r2:.4f}\")\n",
        "\n",
        "    # --- 최종 모델 저장 (sign-quantised, mask 포함) ---\n",
        "    torch.save({\n",
        "        \"mask_fc1\": net[\"fc1\"].mask.cpu(),\n",
        "        \"w_fc1_sign\": torch.sign(net[\"fc1\"].w_real).cpu().to(torch.int8),\n",
        "        \"mask_fc2\": net[\"fc2\"].mask.cpu(),\n",
        "        \"w_fc2_sign\": torch.sign(net[\"fc2\"].w_real).cpu().to(torch.int8),\n",
        "        \"x_scaler_min\": x_scaler.min_,\n",
        "        \"x_scaler_scale\": x_scaler.scale_,\n",
        "        \"y_scaler_mean\": y_scaler.mean_,\n",
        "        \"y_scaler_scale\": y_scaler.scale_,\n",
        "    }, \"sqeh_sparse_bin.pt\")\n",
        "\n",
        "    print(\"\\n학습 완료. 희소-양자화된 가중치 저장됨 → sqeh_sparse_bin.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리"
      ],
      "metadata": {
        "id": "fzYM-0NBqalI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gwnk-iVtUTB",
        "outputId": "bce5d064-d81c-40b7-c1d6-f80921b7b96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snntorch\n",
            "Successfully installed snntorch-0.9.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU 확인"
      ],
      "metadata": {
        "id": "MK0u3GHKqdau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, platform, os\n",
        "\n",
        "print(\"CUDA available? ➜\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"PyTorch CUDA version:\", torch.version.cuda)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOnEIHJstmHy",
        "outputId": "3f9cbf2e-3fb2-40c4-9ffd-d717e2ec5765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available? ➜ True\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "PyTorch CUDA version: 12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리 및 모델 개발(GRN 모방, 경량화X)"
      ],
      "metadata": {
        "id": "4QWxnmbtqjqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ImmunoGRN Studio\n",
        "- 인공 면역체계 기반 이상치 분석 및 GRN 기반 앙상블 모델 학습/평가 파이프라인\n",
        "- GUI는 Tkinter를 사용하여 두 기능(이상치 분석 / GRN 앙상블 모델)을 탭으로 제공\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import datetime\n",
        "import threading\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn 관련\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TensorFlow 및 Keras 관련\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model  # type: ignore\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input, Layer, Add  # type: ignore\n",
        "from tensorflow.keras.optimizers import Adam  # type: ignore\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard  # type: ignore\n",
        "\n",
        "# SHAP\n",
        "import shap\n",
        "\n",
        "# Keras Tuner 설치 확인\n",
        "try:\n",
        "    import keras_tuner\n",
        "except ImportError:\n",
        "    raise ImportError(\"keras-tuner를 설치해야 합니다.\\n예: pip install keras-tuner\")\n",
        "\n",
        "# Tkinter (GUI)\n",
        "import tkinter as tk\n",
        "from tkinter import ttk, filedialog, messagebox\n",
        "\n",
        "##############################################\n",
        "# 1. 인공 면역체계 기반 이상치 분석 함수들\n",
        "##############################################\n",
        "\n",
        "\n",
        "def initialize_detectors(data, num_detectors=200):\n",
        "    \"\"\"\n",
        "    초기 검출기(detector)를 데이터의 각 특성 범위 내에서 무작위로 생성\n",
        "    \"\"\"\n",
        "    min_vals = np.min(data, axis=0)\n",
        "    max_vals = np.max(data, axis=0)\n",
        "    detectors = np.random.uniform(\n",
        "        low=min_vals, high=max_vals, size=(num_detectors, data.shape[1])\n",
        "    )\n",
        "    return detectors\n",
        "\n",
        "\n",
        "def compute_affinity(detector, data_point):\n",
        "    \"\"\"두 벡터 간 유클리드 거리를 계산\"\"\"\n",
        "    return np.linalg.norm(detector - data_point)\n",
        "\n",
        "\n",
        "def negative_selection_classification(data, detectors, threshold):\n",
        "    \"\"\"\n",
        "    각 데이터 포인트와 모든 검출기 간의 거리를 계산하여,\n",
        "    임계치 이하의 거리가 하나라도 존재하면 정상으로 간주하고, 그렇지 않으면 이상치로 분류\n",
        "    \"\"\"\n",
        "    distances = np.linalg.norm(\n",
        "        data[:, np.newaxis, :] - detectors[np.newaxis, :, :], axis=2\n",
        "    )\n",
        "    is_normal = np.any(distances < threshold, axis=1)\n",
        "    outlier_indices = np.where(~is_normal)[0].tolist()\n",
        "    return outlier_indices\n",
        "\n",
        "\n",
        "def clonal_selection(\n",
        "    detectors, data, threshold, iterations=10, clone_factor=5, mutation_rate=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    클론 선택 알고리즘: 활성화(detector와의 근접 빈도)를 기반으로 선택 후 복제 및 돌연변이 적용,\n",
        "    최종적으로 지정한 수의 검출기를 유지\n",
        "    \"\"\"\n",
        "    desired_num = detectors.shape[0]\n",
        "    for iteration in range(iterations):\n",
        "        distances = np.linalg.norm(\n",
        "            data[np.newaxis, :, :] - detectors[:, np.newaxis, :], axis=2\n",
        "        )\n",
        "        activations = np.sum(distances < threshold, axis=1)\n",
        "        num_to_select = max(1, int(0.5 * len(detectors)))\n",
        "        selected_indices = np.argsort(activations)[-num_to_select:]\n",
        "        selected_detectors = detectors[selected_indices]\n",
        "\n",
        "        clones = []\n",
        "        for detector in selected_detectors:\n",
        "            for _ in range(clone_factor):\n",
        "                mutation = np.random.normal(0, mutation_rate, detector.shape)\n",
        "                clones.append(detector + mutation)\n",
        "        clones = np.array(clones)\n",
        "\n",
        "        detectors = np.vstack((detectors, clones))\n",
        "        distances = np.linalg.norm(\n",
        "            data[np.newaxis, :, :] - detectors[:, np.newaxis, :], axis=2\n",
        "        )\n",
        "        new_activations = np.sum(distances < threshold, axis=1)\n",
        "        top_indices = np.argsort(new_activations)[-desired_num:]\n",
        "        detectors = detectors[top_indices]\n",
        "    return detectors\n",
        "\n",
        "\n",
        "def remove_outliers(data, outlier_indices):\n",
        "    \"\"\"이상치 인덱스를 제외한 데이터를 반환\"\"\"\n",
        "    mask = np.ones(len(data), dtype=bool)\n",
        "    mask[outlier_indices] = False\n",
        "    return data[mask], outlier_indices\n",
        "\n",
        "\n",
        "##############################################\n",
        "# 2. GRN 기반 앙상블 모델 관련 함수 및 클래스\n",
        "##############################################\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"재현성을 위해 시드 설정\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def set_korean_font(font_path=\"C:/Windows/Fonts/malgun.ttf\"):\n",
        "    \"\"\"한글 폰트 설정 (matplotlib)\"\"\"\n",
        "    from matplotlib import font_manager, rc\n",
        "\n",
        "    try:\n",
        "        font = font_manager.FontProperties(fname=font_path).get_name()\n",
        "    except Exception as e:\n",
        "        print(f\"폰트 설정 오류: {e}\\n기본 폰트로 설정합니다.\")\n",
        "        font = \"sans-serif\"\n",
        "    rc(\"font\", family=font)\n",
        "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    엑셀 파일을 읽어 데이터프레임으로 변환 후,\n",
        "    feature와 타겟(y)을 numpy 배열로 분리\n",
        "    \"\"\"\n",
        "    data = pd.read_excel(file_path)\n",
        "    feature_names = data.iloc[0, 1:].tolist()\n",
        "    data = data.iloc[1:].reset_index(drop=True)\n",
        "    data = data.dropna()\n",
        "    X = data.iloc[:, 1:].to_numpy()\n",
        "    y = data.iloc[:, 0].to_numpy()\n",
        "    return X, y, feature_names\n",
        "\n",
        "\n",
        "class GRNLayer(Layer):\n",
        "    \"\"\"\n",
        "    GRN (Gene Regulatory Network) 레이어\n",
        "    - 입력 특성 간 상호작용 및 메모리 효과를 적용하여 결과를 생성\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        memory_size=100,\n",
        "        interaction_strength=0.1,\n",
        "        memory_decay=0.9,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(GRNLayer, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.memory_size = memory_size\n",
        "        self.interaction_strength = interaction_strength\n",
        "        self.memory_decay = memory_decay\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.interaction_matrix = self.add_weight(\n",
        "            shape=(self.input_dim, self.input_dim),\n",
        "            initializer=tf.keras.initializers.RandomNormal(\n",
        "                mean=0.0, stddev=self.interaction_strength\n",
        "            ),\n",
        "            trainable=True,\n",
        "            name=\"interaction_matrix\",\n",
        "        )\n",
        "        self.memory = self.add_weight(\n",
        "            shape=(self.memory_size, self.input_dim),\n",
        "            initializer=tf.keras.initializers.Zeros(),\n",
        "            trainable=False,\n",
        "            name=\"memory\",\n",
        "        )\n",
        "        super(GRNLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        interaction_effect = tf.matmul(inputs, self.interaction_matrix)\n",
        "        memory_effect = tf.reduce_mean(self.memory, axis=0)\n",
        "        combined_effect = interaction_effect + memory_effect\n",
        "        if training:\n",
        "            current_memory = tf.reduce_mean(inputs, axis=0, keepdims=True)\n",
        "            updated_memory = tf.concat([self.memory[1:], current_memory], axis=0)\n",
        "            updated_memory = (\n",
        "                self.memory_decay * updated_memory\n",
        "                + (1 - self.memory_decay) * current_memory\n",
        "            )\n",
        "            self.memory.assign(updated_memory)\n",
        "        return combined_effect\n",
        "\n",
        "\n",
        "def train_baseline_model(X, y, learning_rate=0.0005, batch_size=32):\n",
        "    \"\"\"\n",
        "    베이스라인 모델 학습:\n",
        "    - 데이터를 분할 및 스케일링하고, 모델을 학습한 후 최적의 랜덤 시드를 선택\n",
        "    \"\"\"\n",
        "    best_rmse = float(\"inf\")\n",
        "    best_state = None\n",
        "\n",
        "    for random_state in range(42, 43):\n",
        "        set_seed(random_state)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.25, random_state=random_state\n",
        "        )\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        model = Sequential(\n",
        "            [\n",
        "                Input(shape=(X_train_scaled.shape[1],)),\n",
        "                Dense(512, kernel_initializer=\"he_normal\"),\n",
        "                LeakyReLU(alpha=0.2),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                Dense(128, kernel_initializer=\"he_normal\"),\n",
        "                LeakyReLU(alpha=0.2),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.2),\n",
        "                Dense(64, kernel_initializer=\"he_normal\"),\n",
        "                LeakyReLU(alpha=0.2),\n",
        "                BatchNormalization(),\n",
        "                Dense(1),\n",
        "            ]\n",
        "        )\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate), loss=\"mse\", metrics=[\"mse\"]\n",
        "        )\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=50, restore_best_weights=True\n",
        "        )\n",
        "        lr_scheduler = ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.5, patience=15, min_lr=1e-6\n",
        "        )\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            \"baseline_best_model.h5\", monitor=\"val_loss\", save_best_only=True, verbose=0\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_train_scaled,\n",
        "            y_train,\n",
        "            validation_data=(X_test_scaled, y_test),\n",
        "            epochs=1000,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "            callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
        "        )\n",
        "\n",
        "        y_pred_test = model.predict(X_test_scaled).flatten()\n",
        "        rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "        if rmse_test < best_rmse:\n",
        "            best_rmse = rmse_test\n",
        "            best_state = random_state\n",
        "\n",
        "    print(f\"[베이스라인] Best Random State: {best_state}, Best RMSE: {best_rmse:.4f}\")\n",
        "    set_seed(best_state)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=best_state\n",
        "    )\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, best_state\n",
        "\n",
        "\n",
        "def tuner_model_builder(hp):\n",
        "    \"\"\"\n",
        "    Keras Tuner를 위한 하이퍼모델 빌더\n",
        "    - 히든 레이어 개수, 노드 수, dropout 비율, 학습률 등을 튜닝함\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(X_train_scaled_global.shape[1],))\n",
        "    x = inputs\n",
        "    n_layers = hp.Int(\"n_layers\", min_value=2, max_value=4, step=1)\n",
        "    for i in range(n_layers):\n",
        "        units = hp.Choice(f\"units_{i}\", values=[64, 128, 256, 512], default=128)\n",
        "        x = Dense(units, kernel_initializer=\"he_normal\")(x)\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        dropout_rate = hp.Float(\n",
        "            f\"dropout_rate_{i}\", min_value=0.0, max_value=0.5, step=0.1\n",
        "        )\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    tuned_lr = hp.Float(\n",
        "        \"learning_rate\", min_value=1e-5, max_value=5e-3, sampling=\"log\", default=1e-4\n",
        "    )\n",
        "    model.compile(optimizer=Adam(learning_rate=tuned_lr), loss=\"mse\", metrics=[\"mse\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_model_from_best_hps(\n",
        "    best_hps, input_shape, use_grn=False, grn_layer_instance=None\n",
        "):\n",
        "    \"\"\"\n",
        "    튜닝된 하이퍼파라미터(best_hps)를 바탕으로 모델을 구축\n",
        "    - GRNLayer를 적용할지 여부를 선택할 수 있음\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    if use_grn and grn_layer_instance is not None:\n",
        "        grn_output = grn_layer_instance(inputs)\n",
        "        x = Add()([inputs, grn_output])\n",
        "    else:\n",
        "        x = inputs\n",
        "    n_layers = best_hps.get(\"n_layers\")\n",
        "    for i in range(n_layers):\n",
        "        units = best_hps.get(f\"units_{i}\")\n",
        "        dropout_rate = best_hps.get(f\"dropout_rate_{i}\")\n",
        "        x = Dense(units, kernel_initializer=\"he_normal\")(x)\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    tuned_lr = best_hps.get(\"learning_rate\")\n",
        "    model.compile(optimizer=Adam(learning_rate=tuned_lr), loss=\"mse\", metrics=[\"mse\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_ensemble_models(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    X_test_scaled,\n",
        "    y_test,\n",
        "    best_hps,\n",
        "    num_models=3,\n",
        "    batch_size=32,\n",
        "    memory_size=100,\n",
        "    interaction_strength=0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    앙상블 모델 학습:\n",
        "    - 첫 번째 모델에는 GRNLayer 적용, 이후 모델은 일반 모델로 학습\n",
        "    - 각 모델은 별도의 체크포인트와 TensorBoard 로그를 기록\n",
        "    \"\"\"\n",
        "    models = []\n",
        "    grn_layer_instance = GRNLayer(\n",
        "        input_dim=X_train_scaled.shape[1],\n",
        "        memory_size=memory_size,\n",
        "        interaction_strength=interaction_strength,\n",
        "    )\n",
        "    checkpoint_dir = \"ensemble_checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    for i in range(num_models):\n",
        "        if i == 0:\n",
        "            print(f\"\\n[학습 시작] 모델 {i+1}/{num_models} (GRNLayer 적용)\")\n",
        "            model = build_model_from_best_hps(\n",
        "                best_hps,\n",
        "                input_shape=(X_train_scaled.shape[1],),\n",
        "                use_grn=True,\n",
        "                grn_layer_instance=grn_layer_instance,\n",
        "            )\n",
        "        else:\n",
        "            print(f\"\\n[학습 시작] 모델 {i+1}/{num_models} (GRNLayer 미적용)\")\n",
        "            model = build_model_from_best_hps(\n",
        "                best_hps, input_shape=(X_train_scaled.shape[1],), use_grn=False\n",
        "            )\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor=\"val_loss\", patience=50, restore_best_weights=True\n",
        "        )\n",
        "        lr_scheduler = ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.5, patience=15, min_lr=1e-6\n",
        "        )\n",
        "        model_checkpoint = ModelCheckpoint(\n",
        "            os.path.join(checkpoint_dir, f\"model_{i+1}.h5\"),\n",
        "            monitor=\"val_loss\",\n",
        "            save_best_only=True,\n",
        "            verbose=0,\n",
        "        )\n",
        "        tensorboard_cb = TensorBoard(log_dir=os.path.join(log_dir, f\"model_{i+1}\"))\n",
        "        model.fit(\n",
        "            X_train_scaled,\n",
        "            y_train,\n",
        "            validation_data=(X_test_scaled, y_test),\n",
        "            epochs=1000,\n",
        "            batch_size=batch_size,\n",
        "            verbose=1,\n",
        "            callbacks=[early_stopping, lr_scheduler, model_checkpoint, tensorboard_cb],\n",
        "        )\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "\n",
        "def evaluate_ensemble(models, X_test_scaled, y_test, use_weighted=False):\n",
        "    \"\"\"\n",
        "    앙상블 모델 평가:\n",
        "    - 각 모델 예측 결과를 평균하여 최종 예측 도출 후 평가 지표(R^2, MSE, RMSE)를 출력\n",
        "    \"\"\"\n",
        "    predictions_list = [model.predict(X_test_scaled).flatten() for model in models]\n",
        "    predictions_array = np.array(predictions_list)\n",
        "    if use_weighted:\n",
        "        weights = np.ones(len(models)) / len(models)\n",
        "        ensemble_predictions = np.average(predictions_array, axis=0, weights=weights)\n",
        "    else:\n",
        "        ensemble_predictions = np.mean(predictions_array, axis=0)\n",
        "    mse_test = mean_squared_error(y_test, ensemble_predictions)\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    r2_test = r2_score(y_test, ensemble_predictions)\n",
        "    print(\"\\n[최종 앙상블 결과]\")\n",
        "    print(f\"Test R^2: {r2_test:.4f}, MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
        "    return ensemble_predictions, mse_test, rmse_test, r2_test\n",
        "\n",
        "\n",
        "def run_shap_analysis(model, X_train_scaled, X_test_scaled, feature_names):\n",
        "    \"\"\"\n",
        "    SHAP 분석:\n",
        "    - 첫 번째 모델 기준으로 SHAP 값 및 요약, force plot을 생성하여 시각화\n",
        "    \"\"\"\n",
        "    print(\"\\n[SHAP 분석] 첫 번째 모델 기준\")\n",
        "    background = X_train_scaled[\n",
        "        np.random.choice(X_train_scaled.shape[0], 100, replace=False)\n",
        "    ]\n",
        "    explainer = shap.Explainer(model, background)\n",
        "    shap_values = explainer(X_test_scaled)\n",
        "    shap.summary_plot(shap_values, X_test_scaled, feature_names=feature_names)\n",
        "    shap.initjs()\n",
        "    sample_features = pd.Series(X_test_scaled[0, :], index=feature_names)\n",
        "    force_plot = shap.force_plot(\n",
        "        shap_values[0].base_values,\n",
        "        shap_values[0].values,\n",
        "        sample_features,\n",
        "        matplotlib=True,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_tf_dataset(X, y, batch_size=32, shuffle=True):\n",
        "    \"\"\"tf.data.Dataset 객체 생성 (batch 및 prefetch 적용)\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X))\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "##############################################\n",
        "# 3. GUI 통합: 이상치 분석 및 GRN 앙상블 모델 탭 제공\n",
        "##############################################\n",
        "\n",
        "\n",
        "class OutlierRemovalGUI(ttk.Frame):\n",
        "    \"\"\"이상치 분석을 위한 GUI 탭\"\"\"\n",
        "\n",
        "    def __init__(self, container):\n",
        "        super().__init__(container, padding=15)\n",
        "        self.file_path = \"\"\n",
        "        # 파일 선택 위젯\n",
        "        ttk.Label(self, text=\"Excel 파일 선택:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=0, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.file_entry = ttk.Entry(self, width=50)\n",
        "        self.file_entry.grid(row=0, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "        ttk.Button(self, text=\"찾아보기\", command=self.browse_file).grid(\n",
        "            row=0, column=2, padx=5, pady=5\n",
        "        )\n",
        "\n",
        "        # 매개변수 입력 위젯\n",
        "        params = [\n",
        "            (\"임계치\", \"1.5\"),\n",
        "            (\"초기 검출기 수\", \"200\"),\n",
        "            (\"클론 선택 반복 횟수\", \"10\"),\n",
        "            (\"Clone Factor\", \"5\"),\n",
        "            (\"Mutation Rate\", \"0.1\"),\n",
        "        ]\n",
        "        self.param_entries = {}\n",
        "        for i, (label_text, default) in enumerate(params, start=1):\n",
        "            ttk.Label(self, text=label_text + \":\", font=(\"맑은 고딕\", 10)).grid(\n",
        "                row=i, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "            )\n",
        "            entry = ttk.Entry(self)\n",
        "            entry.insert(0, default)\n",
        "            entry.grid(row=i, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "            self.param_entries[label_text] = entry\n",
        "\n",
        "        # 실행 버튼 및 로그 출력 영역\n",
        "        ttk.Button(\n",
        "            self, text=\"이상치 분석 및 처리\", command=self.run_outlier_removal\n",
        "        ).grid(row=6, column=0, columnspan=3, pady=10)\n",
        "        self.log_text = tk.Text(self, height=10, width=70, font=(\"맑은 고딕\", 9))\n",
        "        self.log_text.grid(row=7, column=0, columnspan=3, padx=5, pady=5, sticky=\"nsew\")\n",
        "\n",
        "        self.columnconfigure(1, weight=1)\n",
        "        self.rowconfigure(7, weight=1)\n",
        "\n",
        "    def browse_file(self):\n",
        "        \"\"\"파일 탐색기 호출 후 파일 경로 설정\"\"\"\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            filetypes=[(\"Excel 파일\", \"*.xlsx *.xls\")]\n",
        "        )\n",
        "        if file_path:\n",
        "            self.file_path = file_path\n",
        "            self.file_entry.delete(0, tk.END)\n",
        "            self.file_entry.insert(0, file_path)\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"로그 메시지 출력\"\"\"\n",
        "        self.log_text.insert(tk.END, message + \"\\n\")\n",
        "        self.log_text.see(tk.END)\n",
        "        self.update_idletasks()\n",
        "\n",
        "    def run_outlier_removal(self):\n",
        "        if not self.file_path:\n",
        "            messagebox.showerror(\"오류\", \"Excel 파일을 선택하세요.\")\n",
        "            return\n",
        "        threading.Thread(target=self.process_file).start()\n",
        "\n",
        "    def process_file(self):\n",
        "        try:\n",
        "            self.log(\"Excel 파일 읽는 중...\")\n",
        "            df = pd.read_excel(self.file_path)\n",
        "            data = df.values.astype(float)\n",
        "        except Exception as e:\n",
        "            messagebox.showerror(\"오류\", f\"Excel 파일 읽기 오류: {e}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            threshold = float(self.param_entries[\"임계치\"].get())\n",
        "            detectors_num = int(self.param_entries[\"초기 검출기 수\"].get())\n",
        "            iterations = int(self.param_entries[\"클론 선택 반복 횟수\"].get())\n",
        "            clone_factor = int(self.param_entries[\"Clone Factor\"].get())\n",
        "            mutation_rate = float(self.param_entries[\"Mutation Rate\"].get())\n",
        "        except Exception as e:\n",
        "            messagebox.showerror(\"오류\", f\"매개변수 변환 오류: {e}\")\n",
        "            return\n",
        "\n",
        "        self.log(\"초기 검출기 생성 중...\")\n",
        "        detectors = initialize_detectors(data, num_detectors=detectors_num)\n",
        "        self.log(\"클론 선택 진행 중...\")\n",
        "        detectors = clonal_selection(\n",
        "            detectors, data, threshold, iterations, clone_factor, mutation_rate\n",
        "        )\n",
        "        self.log(\"이상치 분류 중...\")\n",
        "        outlier_indices = negative_selection_classification(data, detectors, threshold)\n",
        "        cleaned_data, _ = remove_outliers(data, outlier_indices)\n",
        "        cleaned_df = pd.DataFrame(cleaned_data, columns=df.columns)\n",
        "\n",
        "        save_path = filedialog.asksaveasfilename(\n",
        "            defaultextension=\".xlsx\",\n",
        "            filetypes=[(\"Excel 파일\", \"*.xlsx\")],\n",
        "            title=\"저장할 파일 선택\",\n",
        "        )\n",
        "        if save_path:\n",
        "            try:\n",
        "                cleaned_df.to_excel(save_path, index=False)\n",
        "                self.log(f\"파일 저장 완료: {save_path}\")\n",
        "                messagebox.showinfo(\n",
        "                    \"완료\", f\"파일이 성공적으로 저장되었습니다:\\n{save_path}\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                messagebox.showerror(\"오류\", f\"파일 저장 오류: {e}\")\n",
        "        else:\n",
        "            self.log(\"파일 저장이 취소되었습니다.\")\n",
        "\n",
        "\n",
        "class GRNEnsembleGUI(ttk.Frame):\n",
        "    \"\"\"GRN 기반 앙상블 모델 학습 및 평가를 위한 GUI 탭\"\"\"\n",
        "\n",
        "    def __init__(self, container):\n",
        "        super().__init__(container, padding=15)\n",
        "        # 파일 선택 위젯\n",
        "        ttk.Label(self, text=\"Excel 파일 선택:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=0, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.file_path = \"\"\n",
        "        self.file_entry = ttk.Entry(self, width=50)\n",
        "        self.file_entry.grid(row=0, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "        ttk.Button(self, text=\"찾아보기\", command=self.browse_file).grid(\n",
        "            row=0, column=2, padx=5, pady=5\n",
        "        )\n",
        "\n",
        "        # 한글 폰트 경로 입력\n",
        "        ttk.Label(self, text=\"한글 폰트 경로:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=1, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.font_entry = ttk.Entry(self, width=50)\n",
        "        self.font_entry.insert(0, \"C:/Windows/Fonts/malgun.ttf\")\n",
        "        self.font_entry.grid(row=1, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        # 베이스라인 학습 파라미터\n",
        "        ttk.Label(self, text=\"Learning Rate (Baseline):\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=2, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.lr_entry = ttk.Entry(self)\n",
        "        self.lr_entry.insert(0, \"0.0005\")\n",
        "        self.lr_entry.grid(row=2, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        ttk.Label(self, text=\"Batch Size:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=3, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.bs_entry = ttk.Entry(self)\n",
        "        self.bs_entry.insert(0, \"32\")\n",
        "        self.bs_entry.grid(row=3, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        # 튜너 파라미터\n",
        "        ttk.Label(self, text=\"Tuner max_epochs:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=4, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.max_epochs_entry = ttk.Entry(self)\n",
        "        self.max_epochs_entry.insert(0, \"230\")\n",
        "        self.max_epochs_entry.grid(row=4, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        # 앙상블 모델 파라미터\n",
        "        ttk.Label(self, text=\"앙상블 모델 수:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=5, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.num_models_entry = ttk.Entry(self)\n",
        "        self.num_models_entry.insert(0, \"3\")\n",
        "        self.num_models_entry.grid(row=5, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        ttk.Label(self, text=\"Memory Size:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=6, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.memory_size_entry = ttk.Entry(self)\n",
        "        self.memory_size_entry.insert(0, \"100\")\n",
        "        self.memory_size_entry.grid(row=6, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        ttk.Label(self, text=\"Interaction Strength:\", font=(\"맑은 고딕\", 10)).grid(\n",
        "            row=7, column=0, padx=5, pady=5, sticky=\"w\"\n",
        "        )\n",
        "        self.int_strength_entry = ttk.Entry(self)\n",
        "        self.int_strength_entry.insert(0, \"0.1\")\n",
        "        self.int_strength_entry.grid(row=7, column=1, padx=5, pady=5, sticky=\"ew\")\n",
        "\n",
        "        # 실행 버튼 및 로그 출력 영역\n",
        "        ttk.Button(\n",
        "            self, text=\"전체 파이프라인 실행\", command=self.start_pipeline_thread\n",
        "        ).grid(row=8, column=0, columnspan=3, pady=10)\n",
        "        self.log_text = tk.Text(self, height=15, width=70, font=(\"맑은 고딕\", 9))\n",
        "        self.log_text.grid(row=9, column=0, columnspan=3, padx=5, pady=5, sticky=\"nsew\")\n",
        "\n",
        "        self.columnconfigure(1, weight=1)\n",
        "        self.rowconfigure(9, weight=1)\n",
        "\n",
        "    def browse_file(self):\n",
        "        \"\"\"파일 탐색기 호출 후 파일 경로 설정\"\"\"\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            filetypes=[(\"Excel 파일\", \"*.xlsx *.xls\")]\n",
        "        )\n",
        "        if file_path:\n",
        "            self.file_path = file_path\n",
        "            self.file_entry.delete(0, tk.END)\n",
        "            self.file_entry.insert(0, file_path)\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"로그 메시지 출력\"\"\"\n",
        "        self.log_text.insert(tk.END, message + \"\\n\")\n",
        "        self.log_text.see(tk.END)\n",
        "        self.update_idletasks()\n",
        "\n",
        "    def start_pipeline_thread(self):\n",
        "        threading.Thread(target=self.run_pipeline).start()\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        try:\n",
        "            font_path = self.font_entry.get().strip()\n",
        "            self.log(\"한글 폰트 설정 중...\")\n",
        "            set_korean_font(font_path=font_path)\n",
        "            if not self.file_path:\n",
        "                messagebox.showerror(\"오류\", \"Excel 파일을 선택하세요.\")\n",
        "                return\n",
        "            self.log(\"엑셀 파일 로드 중...\")\n",
        "            X, y, feature_names = load_and_preprocess_data(self.file_path)\n",
        "            self.log(\"데이터 로드 완료.\")\n",
        "            lr = float(self.lr_entry.get().strip())\n",
        "            bs = int(self.bs_entry.get().strip())\n",
        "            self.log(\"베이스라인 모델 학습 시작...\")\n",
        "            X_train_scaled, X_test_scaled, y_train, y_test, scaler, best_state = (\n",
        "                train_baseline_model(X, y, learning_rate=lr, batch_size=bs)\n",
        "            )\n",
        "            self.log(f\"[베이스라인] Best Random State: {best_state}\")\n",
        "            global X_train_scaled_global\n",
        "            X_train_scaled_global = X_train_scaled\n",
        "            self.log(\"Keras Tuner 하이퍼파라미터 탐색 시작...\")\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            self.log(f\"사용 가능한 디바이스 수: {strategy.num_replicas_in_sync}\")\n",
        "            max_epochs = int(self.max_epochs_entry.get().strip())\n",
        "            with strategy.scope():\n",
        "                tuner = keras_tuner.Hyperband(\n",
        "                    hypermodel=tuner_model_builder,\n",
        "                    objective=\"val_loss\",\n",
        "                    max_epochs=max_epochs,\n",
        "                    factor=3,\n",
        "                    directory=\"my_dir\",\n",
        "                    project_name=\"grn_tuning\",\n",
        "                    overwrite=True,\n",
        "                )\n",
        "            stop_early = EarlyStopping(\n",
        "                monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
        "            )\n",
        "            tuner.search(\n",
        "                X_train_scaled,\n",
        "                y_train,\n",
        "                epochs=70,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[stop_early],\n",
        "                verbose=1,\n",
        "            )\n",
        "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "            self.log(\"튜너 결과: \" + str(best_hps.values))\n",
        "            train_dataset = create_tf_dataset(\n",
        "                X_train_scaled, y_train, batch_size=bs, shuffle=True\n",
        "            )\n",
        "            test_dataset = create_tf_dataset(\n",
        "                X_test_scaled, y_test, batch_size=bs, shuffle=False\n",
        "            )\n",
        "            num_models = int(self.num_models_entry.get().strip())\n",
        "            mem_size = int(self.memory_size_entry.get().strip())\n",
        "            int_strength = float(self.int_strength_entry.get().strip())\n",
        "            self.log(\"앙상블 모델 학습 시작...\")\n",
        "            models = train_ensemble_models(\n",
        "                X_train_scaled,\n",
        "                y_train,\n",
        "                X_test_scaled,\n",
        "                y_test,\n",
        "                best_hps,\n",
        "                num_models=num_models,\n",
        "                batch_size=bs,\n",
        "                memory_size=mem_size,\n",
        "                interaction_strength=int_strength,\n",
        "            )\n",
        "            self.log(\"앙상블 모델 평가 중...\")\n",
        "            ensemble_predictions, mse_test, rmse_test, r2_test = evaluate_ensemble(\n",
        "                models, X_test_scaled, y_test, use_weighted=False\n",
        "            )\n",
        "            self.log(\n",
        "                f\"[최종 앙상블 결과] Test R^2: {r2_test:.4f}, MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}\"\n",
        "            )\n",
        "            self.log(\"SHAP 분석 시작...\")\n",
        "            run_shap_analysis(models[0], X_train_scaled, X_test_scaled, feature_names)\n",
        "            for i, model in enumerate(models):\n",
        "                model.save(f\"final_model_{i+1}.h5\")\n",
        "            self.log(\"모델 저장 완료.\")\n",
        "            self.log(\"전체 파이프라인 실행 완료.\")\n",
        "        except Exception as e:\n",
        "            self.log(\"오류 발생: \" + str(e))\n",
        "            messagebox.showerror(\"오류\", str(e))\n",
        "\n",
        "\n",
        "##############################################\n",
        "# 4. 메인: Tkinter Notebook을 이용한 GUI 실행\n",
        "##############################################\n",
        "def main():\n",
        "    root = tk.Tk()\n",
        "    root.title(\"ImmunoGRN Studio\")\n",
        "    root.geometry(\"525x600\")\n",
        "\n",
        "    root.columnconfigure(0, weight=1)\n",
        "    root.rowconfigure(0, weight=1)\n",
        "\n",
        "    style = ttk.Style(root)\n",
        "    style.theme_use(\"clam\")\n",
        "    style.configure(\n",
        "        \"TEntry\",\n",
        "        font=(\"맑은 고딕\", 10),\n",
        "        foreground=\"black\",\n",
        "        fieldbackground=\"#F7F9FC\",\n",
        "        background=\"#F7F9FC\",\n",
        "        borderwidth=1,\n",
        "        relief=\"flat\",\n",
        "    )\n",
        "    default_font = (\"맑은 고딕\", 10)\n",
        "    style.configure(\".\", font=default_font)\n",
        "\n",
        "    notebook = ttk.Notebook(root)\n",
        "    notebook.pack(expand=True, fill=\"both\")\n",
        "\n",
        "    tab1 = OutlierRemovalGUI(notebook)\n",
        "    notebook.add(tab1, text=\"이상치 분석\")\n",
        "    tab2 = GRNEnsembleGUI(notebook)\n",
        "    notebook.add(tab2, text=\"GRN 앙상블 모델\")\n",
        "\n",
        "    root.mainloop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "5gxdMut3qiYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
